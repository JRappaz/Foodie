{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AllRecipes.com scraper\n",
    "\n",
    "This python notebook is used to scrape recipes and users of the allrecipes.com platform in order to create a user/item matrix that will be used to design a recipes recommender system based on ingredients, recipe style, user preferences, reviews, ratings, etc...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import *\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from util import *\n",
    "%matplotlib inline\n",
    "\n",
    "endpoint = 'http://allrecipes.com/recipes/?grouping=all&page='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsePage(page):\n",
    "    articlesList = []\n",
    "    articles = page.find_all(\"article\")\n",
    "    for article in articles:\n",
    "        if article.h3 is not None:\n",
    "            name = removeSpace(article.h3.text)\n",
    "            if name != \"\":\n",
    "                print(name)\n",
    "                try:\n",
    "                    ratingStars = article.find(\"div\",{\"class\" : \"rating-stars\"})[\"data-ratingstars\"]\n",
    "                    nbReviews = article.find(\"format-large-number\")[\"number\"]\n",
    "                except:\n",
    "                    ratingStars = None\n",
    "                    nbReviews = None\n",
    "                link = article.find('a', href=re.compile('^/recipe/'))\n",
    "                if link is not None:\n",
    "                    print(link['href'])\n",
    "                    articlesList.append([name, ratingStars, nbReviews, link['href']])\n",
    "                else:\n",
    "                    link = article.find('a', href=re.compile('^/cook/'))\n",
    "                    if link is not None:\n",
    "                        print(link['href'])\n",
    "                        articlesList.append([name, ratingStars, nbReviews, link['href']])\n",
    "                    else:\n",
    "                        print(\"REALLY NO LINKS\")\n",
    "    return articlesList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'articlesList = []\\nfor i in range(1, 2):\\n    response = urlopen(endpoint + str(i))\\n    html = response.read()\\n    page = BeautifulSoup(html, \\'html.parser\\')\\n    articlesList += parsePage(page)\\n    print(\"Scraping page \" + str(i))\\n    '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''articlesList = []\n",
    "for i in range(1, 2):\n",
    "    response = urlopen(endpoint + str(i))\n",
    "    html = response.read()\n",
    "    page = BeautifulSoup(html, 'html.parser')\n",
    "    articlesList += parsePage(page)\n",
    "    print(\"Scraping page \" + str(i))\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'articles = pd.DataFrame(articlesList)\\narticles.columns=[\"Recipe title\", \"Rating\", \"Reviews\", \"Recipe link\"]\\narticles'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''articles = pd.DataFrame(articlesList)\n",
    "articles.columns=[\"Recipe title\", \"Rating\", \"Reviews\", \"Recipe link\"]\n",
    "articles'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'articles.to_csv(\"indexes/RecipeLinks.csv\", index=False, encoding=\\'utf-8\\')'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''articles.to_csv(\"indexes/RecipeLinks.csv\", index=False, encoding='utf-8')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merging all links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'articles_1000 = pd.read_csv(\"indexes/1000_RecipeLinks.csv\")\\narticles_2000 = pd.read_csv(\"indexes/2000_RecipeLinks.csv\")\\narticles_3000 = pd.read_csv(\"indexes/3000_RecipeLinks.csv\")\\nall_articles = pd.concat([articles_1000, articles_2000, articles_3000])\\nall_articles.to_csv(\"indexes/RecipeLinks.csv\", index=False, encoding=\\'utf-8\\')\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''articles_1000 = pd.read_csv(\"indexes/1000_RecipeLinks.csv\")\n",
    "articles_2000 = pd.read_csv(\"indexes/2000_RecipeLinks.csv\")\n",
    "articles_3000 = pd.read_csv(\"indexes/3000_RecipeLinks.csv\")\n",
    "all_articles = pd.concat([articles_1000, articles_2000, articles_3000])\n",
    "all_articles.to_csv(\"indexes/RecipeLinks.csv\", index=False, encoding='utf-8')\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing links and number of reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_articles = pd.read_csv(\"indexes/RecipeLinks.csv\")\n",
    "cleaned_articles = all_articles.dropna().reset_index()\n",
    "del cleaned_articles['index']\n",
    "reviews = cleaned_articles[\"Reviews\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>59800.000000</td>\n",
       "      <td>59800.000000</td>\n",
       "      <td>59800.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>29912.495652</td>\n",
       "      <td>3.901782</td>\n",
       "      <td>65.999532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>17262.924914</td>\n",
       "      <td>1.323218</td>\n",
       "      <td>294.742222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>14962.750000</td>\n",
       "      <td>3.970000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>29912.500000</td>\n",
       "      <td>4.330000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>44862.250000</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>59812.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10833.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Unnamed: 0        Rating       Reviews\n",
       "count  59800.000000  59800.000000  59800.000000\n",
       "mean   29912.495652      3.901782     65.999532\n",
       "std    17262.924914      1.323218    294.742222\n",
       "min        0.000000      0.000000      0.000000\n",
       "25%    14962.750000      3.970000      3.000000\n",
       "50%    29912.500000      4.330000     12.000000\n",
       "75%    44862.250000      4.600000     42.000000\n",
       "max    59812.000000      5.000000  10833.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_articles.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recipes with more than 50 reviews: 13089\n",
      "Recipes with more than 100 reviews: 7341\n",
      "Recipes with more than 1000 reviews: 507\n"
     ]
    }
   ],
   "source": [
    "print(\"Recipes with more than 50 reviews: \" + str(len(cleaned_articles[cleaned_articles[\"Reviews\"] > 50])))\n",
    "print(\"Recipes with more than 100 reviews: \" + str(len(cleaned_articles[cleaned_articles[\"Reviews\"] > 100])))\n",
    "print(\"Recipes with more than 1000 reviews: \" + str(len(cleaned_articles[cleaned_articles[\"Reviews\"] > 1000])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA00AAAHnCAYAAABkG7aNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAHfBJREFUeJzt3X+w5Xdd3/HX26wgRUtCslBMMl3U\n1BLb8sMYYtUWCYbAWpJxCAatRMyYSqPiVKuL4zQVSV3GVmxawKYmJqI1higSXGrMoClQAbPhd0hD\nVlxhTYYsJkaQigbe/eN+l17C3c/eu7nnnnPvPh4zO/d8P+d7vt/P2f3OmTzz/Z7vre4OAAAAK/uS\neU8AAABgkYkmAACAAdEEAAAwIJoAAAAGRBMAAMCAaAIAABgQTQAAAAOiCQAAYEA0AQAADGyb9wRm\n4aSTTuodO3bMexoAAMACu+222z7R3duPtN6WjKYdO3Zk7969854GAACwwKrqT1eznsvzAAAABkQT\nAADAgGgCAAAYEE0AAAADogkAAGBANAEAAAyIJgAAgAHRBAAAMCCaAAAABkQTAADAgGgCAAAYEE0A\nAAADogkAAGBANAEAAAyIJgAAgAHRBAAAMCCaAAAABkQTAADAgGgCAAAY2DbvCQCLb8euPXPZ7/7d\nO+eyXwCA5ZxpAgAAGBBNAAAAA6IJAABgQDQBAAAMiCYAAIAB0QQAADAgmgAAAAZEEwAAwIBoAgAA\nGBBNAAAAA6IJAABgQDQBAAAMiCYAAIAB0QQAADAgmgAAAAZEEwAAwIBoAgAAGBBNAAAAA6IJAABg\nQDQBAAAMiCYAAIAB0QQAADAgmgAAAAZmGk1Vtb+qPlBV762qvdPYY6vq5qq6a/p5wjReVXVFVe2r\nqvdX1dOWbeeiaf27quqiWc4ZAABguY040/St3f2U7j5jWt6V5C3dfVqSt0zLSfKcJKdNfy5J8tpk\nKbKSXJbk6UnOTHLZodACAACYtXlcnndekmunx9cmOX/Z+K/0kncmOb6qnpDk2Ulu7u77uvv+JDcn\nOXejJw0AABybZh1NneT3quq2qrpkGnt8d9+TJNPPx03jJyf52LLXHpjGDjf+BarqkqraW1V7Dx48\nuM5vAwAAOFZtm/H2v6m7766qxyW5uar+z2DdWmGsB+NfONB9ZZIrk+SMM874oucBAACOxkzPNHX3\n3dPPe5O8IUvfSfr4dNldpp/3TqsfSHLqspefkuTuwTgAAMDMzSyaqurRVfUVhx4nOSfJB5PcmOTQ\nHfAuSvLG6fGNSV403UXvrCQPTJfv3ZTknKo6YboBxDnTGAAAwMzN8vK8xyd5Q1Ud2s//6O7frapb\nk1xfVRcn+WiSC6b135zkuUn2Jfl0khcnSXffV1U/k+TWab2Xd/d9M5w3AADA580smrr7I0mevML4\nnyc5e4XxTnLpYbZ1dZKr13uOAAAARzLrG0EAHLUdu/bMZb/7d++cy34BgMU0j9/TBAAAsGmIJgAA\ngAHRBAAAMCCaAAAABkQTAADAgGgCAAAYEE0AAAADogkAAGBANAEAAAyIJgAAgAHRBAAAMCCaAAAA\nBkQTAADAgGgCAAAYEE0AAAADogkAAGBANAEAAAyIJgAAgAHRBAAAMCCaAAAABkQTAADAgGgCAAAY\nEE0AAAADogkAAGBANAEAAAyIJgAAgAHRBAAAMCCaAAAABkQTAADAwLZ5TwBYmx279sx7CgAAxxRn\nmgAAAAZEEwAAwIBoAgAAGBBNAAAAA24EAfAQ87jZxv7dOzd8nwDA6jjTBAAAMCCaAAAABkQTAADA\ngGgCAAAYEE0AAAADogkAAGBANAEAAAyIJgAAgAHRBAAAMCCaAAAABkQTAADAgGgCAAAYEE0AAAAD\nogkAAGBANAEAAAyIJgAAgAHRBAAAMCCaAAAABkQTAADAgGgCAAAYEE0AAAAD2+Y9AQCSHbv2zGW/\n+3fvnMt+AWAzcaYJAABgQDQBAAAMuDxvA8zjshuX3AAAwPpwpgkAAGBANAEAAAyIJgAAgAHRBAAA\nMCCaAAAABkQTAADAgGgCAAAYmHk0VdVxVfWeqvqdafmJVfWuqrqrqn6jqh4xjT9yWt43Pb9j2TZe\nNo3fWVXPnvWcAQAADtmIM00vTXLHsuVXJnlVd5+W5P4kF0/jFye5v7u/JsmrpvVSVacnuTDJ1yU5\nN8lrquq4DZg3AADAbKOpqk5JsjPJL03LleSZSW6YVrk2yfnT4/Om5UzPnz2tf16S67r7M939J0n2\nJTlzlvMGAAA4ZNZnmn4hyY8n+dy0fGKSv+juB6flA0lOnh6fnORjSTI9/8C0/ufHV3gNAADATM0s\nmqrq25Pc2923LR9eYdU+wnOj1yzf3yVVtbeq9h48eHDN8wUAAFjJLM80fVOS51XV/iTXZemyvF9I\ncnxVbZvWOSXJ3dPjA0lOTZLp+cckuW/5+Aqv+bzuvrK7z+juM7Zv377+7wYAADgmzSyauvtl3X1K\nd+/I0o0cfr+7vzvJHyR5/rTaRUneOD2+cVrO9Pzvd3dP4xdOd9d7YpLTkvzRrOYNAACw3LYjr7Lu\nfiLJdVX1iiTvSXLVNH5VktdV1b4snWG6MEm6+/aquj7Jh5I8mOTS7v7sxk8bAAA4Fm1INHX3LUlu\nmR5/JCvc/a67/zrJBYd5/eVJLp/dDAEAAFa2Eb+nCQAAYNMSTQAAAAPz+E4TbAk7du2Z9xQAANgA\nzjQBAAAMiCYAAIAB0QQAADAgmgAAAAZEEwAAwIBoAgAAGBBNAAAAA6IJAABgQDQBAAAMiCYAAIAB\n0QQAADAgmgAAAAZEEwAAwIBoAgAAGBBNAAAAA6IJAABgQDQBAAAMiCYAAIAB0QQAADCwbd4TAGB+\nduzas+H73L9754bvEwAeDmeaAAAABkQTAADAgGgCAAAYEE0AAAADogkAAGBANAEAAAyIJgAAgAHR\nBAAAMCCaAAAABkQTAADAgGgCAAAYEE0AAAADogkAAGBANAEAAAyIJgAAgAHRBAAAMCCaAAAABkQT\nAADAgGgCAAAYEE0AAAADogkAAGBANAEAAAyIJgAAgAHRBAAAMCCaAAAABkQTAADAgGgCAAAYEE0A\nAAADogkAAGBANAEAAAyIJgAAgAHRBAAAMCCaAAAABkQTAADAgGgCAAAY2DbvCTAbO3btmct+9+/e\nOZf9AgDArDjTBAAAMCCaAAAABkQTAADAgGgCAAAYEE0AAAADogkAAGBANAEAAAysKpqq6qVV9Xdr\nyVVV9e6qOmfWkwMAAJi31Z5p+r7u/ssk5yTZnuTFSXbPbFYAAAALYrXRVNPP5yb55e5+37IxAACA\nLWu10XRbVf1elqLppqr6iiSfG72gqr6sqv6oqt5XVbdX1U9P40+sqndV1V1V9RtV9Yhp/JHT8r7p\n+R3LtvWyafzOqnr20bxRAACAo7HaaLo4ya4k39Ddn07yiCxdojfymSTP7O4nJ3lKknOr6qwkr0zy\nqu4+Lcn907YP7eP+7v6aJK+a1ktVnZ7kwiRfl+TcJK+pquNWOW8AAICHZbXR1ElOT/LD0/Kjk3zZ\n8AVLPjUtfun0p5M8M8kN0/i1Sc6fHp83LWd6/uyqqmn8uu7+THf/SZJ9Sc5c5bwBAAAeltVG02uS\nfGOSF07Ln0zy6iO9qKqOq6r3Jrk3yc1J/jjJX3T3g9MqB5KcPD0+OcnHkmR6/oEkJy4fX+E1y/d1\nSVXtraq9Bw8eXOXbAgAAGFttND29uy9N8tdJ0t33Z+kSvaHu/mx3PyXJKVk6O/SklVabfq50Y4ke\njD90X1d29xndfcb27duPNDUAAIBVWW00/e30PaJOkqraniPcCGK57v6LJLckOSvJ8VW1bXrqlCR3\nT48PJDl12v62JI9Jct/y8RVeAwAAMFOrjaYrkrwhyeOr6vIkb0/yH0YvqKrtVXX89PhRSZ6V5I4k\nf5Dk+dNqFyV54/T4xmk50/O/3909jV843V3viUlOS/JHq5w3AADAw7LtyKsk3f1rVXVbkrOnofO7\n+44jvOwJSa6dzlB9SZLru/t3qupDSa6rqlckeU+Sq6b1r0ryuqral6UzTBdO+769qq5P8qEkDya5\ntLs/u/q3CAAAcPRWFU2Tv5Pk0CV6jzrSyt39/iRPXWH8I1nh7nfd/ddJLjjMti5Pcvka5goAALAu\nVhVNVfXvshQ0v5mlGzP8clW9vrtfMcvJsfns2LVnLvvdv3vnXPYLAMDWt9ozTS9M8tTpbFCqaneS\ndycRTQAAwJa22htB7M8X/jLbR2bpdy4BAABsaas90/SZJLdX1c1Z+k7TtyV5e1VdkSTd/cMzmh8A\nAMBcrTaa3jD9OeSW9Z8KAADA4lntLcevnfVEAAAAFtEwmqrq+u5+QVV9IEuX5X2B7v4nM5sZAADA\nAjjSmaaXTj+/fdYTAeDY4FcTALDZDKOpu++ZHn5JknuW3XL8UUkeP+O5AQAAzN1qbzn++iSfW7b8\n2WkMAABgS1ttNG3r7r85tDA9fsRspgQAALA4VhtNB6vqeYcWquq8JJ+YzZQAAAAWx2p/T9MPJPm1\nqnp1lu6idyDJi2Y2KwAAgAWx2t/T9MdJzqqqL09S3f3J2U4LAABgMazq8ryqenxVXZXk9d39yao6\nvaounvHcAAAA5m6132m6JslNSb5yWv5wkh+ZxYQAAAAWyWqj6aTuvj7Tbce7+8Es3XYcAABgS1tt\nNP1VVZ2YpZtApKrOSvLAzGYFAACwIFZ797x/k+TGJF9dVf87yfYkF8xsVgAAAAtitXfPe3dV/fMk\nX5ukktzZ3X8705kBAAAsgNVenpfufrC7b+/uDyZ5RlXdPMN5AQAALIRhNFXVM6vqw1X1qar61elW\n43uT7E7y2o2ZIgAAwPwc6UzTf0pySZITk9yQ5J1JXtfdX9/dvzXryQEAAMzbkb7T1N19y/T4t6vq\nYHf/5xnPCQAAYGEcKZqOr6rvWLZcy5edbQIAALa6I0XT/0ryL7L0+5lq2XKmMdEEAABsacNo6u4X\nJ0lV/VR3v2J6/Mju/sxGTA4AAGDejnT3vB+vqm9M8vxlw++Y7ZQAAAAWx5Euz7szyQVJvqqq3pbk\njiQnVtXXdvedM58dAADAnB3pluP3J/nJJPuSPCPJFdP4rqr6wxnOCwAAYCEc6UzTuUkuS/LVSX4+\nyfuS/NWh7zoBAABsdcMzTd39k919dpL9SX41S5G1vareXlVv2oD5AQAAzNWRzjQdclN335rk1qp6\nSXd/c1WdNMuJAQAALIIjfacpSdLdP75s8XunsU/MYkIAAACLZFXRtFx3v28WEwEAAFhEa44mAACA\nY4loAgAAGBBNAAAAA6IJAABgQDQBAAAMiCYAAICB1f5yW1hoO3btmfcUAADYopxpAgAAGBBNAAAA\nA6IJAABgQDQBAAAMiCYAAIAB0QQAADDgluMAHBPm9asJ9u/eOZf9ArB+nGkCAAAYEE0AAAADogkA\nAGBANAEAAAyIJgAAgAHRBAAAMCCaAAAABkQTAADAgGgCAAAYEE0AAAADogkAAGBANAEAAAyIJgAA\ngAHRBAAAMCCaAAAABkQTAADAgGgCAAAYEE0AAAADogkAAGBgZtFUVadW1R9U1R1VdXtVvXQaf2xV\n3VxVd00/T5jGq6quqKp9VfX+qnrasm1dNK1/V1VdNKs5AwAAPNQszzQ9mORHu/tJSc5KcmlVnZ5k\nV5K3dPdpSd4yLSfJc5KcNv25JMlrk6XISnJZkqcnOTPJZYdCCwAAYNZmFk3dfU93v3t6/MkkdyQ5\nOcl5Sa6dVrs2yfnT4/OS/EoveWeS46vqCUmeneTm7r6vu+9PcnOSc2c1bwAAgOU25DtNVbUjyVOT\nvCvJ47v7nmQprJI8blrt5CQfW/ayA9PY4cYfuo9LqmpvVe09ePDger8FAADgGDXzaKqqL0/ym0l+\npLv/crTqCmM9GP/Cge4ru/uM7j5j+/btRzdZAACAh5hpNFXVl2YpmH6tu39rGv74dNldpp/3TuMH\nkpy67OWnJLl7MA4AADBzs7x7XiW5Kskd3f3zy566McmhO+BdlOSNy8ZfNN1F76wkD0yX792U5Jyq\nOmG6AcQ50xgAAMDMbZvhtr8pyfck+UBVvXca+8kku5NcX1UXJ/lokgum596c5LlJ9iX5dJIXJ0l3\n31dVP5Pk1mm9l3f3fTOcNwAAwOfNLJq6++1Z+ftISXL2Cut3kksPs62rk1y9frMDAABYnQ25ex4A\nAMBmJZoAAAAGRBMAAMCAaAIAABgQTQAAAAOiCQAAYEA0AQAADIgmAACAAdEEAAAwIJoAAAAGRBMA\nAMCAaAIAABgQTQAAAAOiCQAAYEA0AQAADIgmAACAAdEEAAAwIJoAAAAGRBMAAMCAaAIAABgQTQAA\nAAPb5j0BANjKduzas+H73L9754bvE2Arc6YJAABgQDQBAAAMiCYAAIAB0QQAADAgmgAAAAZEEwAA\nwIBoAgAAGBBNAAAAA6IJAABgQDQBAAAMiCYAAIAB0QQAADAgmgAAAAZEEwAAwIBoAgAAGBBNAAAA\nA6IJAABgQDQBAAAMiCYAAIAB0QQAADAgmgAAAAZEEwAAwIBoAgAAGBBNAAAAA6IJAABgQDQBAAAM\niCYAAIAB0QQAADAgmgAAAAZEEwAAwIBoAgAAGBBNAAAAA6IJAABgQDQBAAAMiCYAAICBbfOeAACw\nvnbs2jOX/e7fvXMu+wWYNWeaAAAABkQTAADAgGgCAAAYEE0AAAADogkAAGBANAEAAAyIJgAAgAHR\nBAAAMCCaAAAABkQTAADAwMyiqaqurqp7q+qDy8YeW1U3V9Vd088TpvGqqiuqal9Vvb+qnrbsNRdN\n699VVRfNar4AAAArmeWZpmuSnPuQsV1J3tLdpyV5y7ScJM9Jctr055Ikr02WIivJZUmenuTMJJcd\nCi0AAICNMLNo6u63JrnvIcPnJbl2enxtkvOXjf9KL3lnkuOr6glJnp3k5u6+r7vvT3JzvjjEAAAA\nZmajv9P0+O6+J0mmn4+bxk9O8rFl6x2Yxg43DgAAsCEW5UYQtcJYD8a/eANVl1TV3qrae/DgwXWd\nHAAAcOza6Gj6+HTZXaaf907jB5Kcumy9U5LcPRj/It19ZXef0d1nbN++fd0nDgAAHJs2OppuTHLo\nDngXJXnjsvEXTXfROyvJA9PlezclOaeqTphuAHHONAYAALAhts1qw1X160mekeSkqjqQpbvg7U5y\nfVVdnOSjSS6YVn9zkucm2Zfk00lenCTdfV9V/UySW6f1Xt7dD725BAAAwMzMLJq6+4WHeersFdbt\nJJceZjtXJ7l6HacGAACwaotyIwgAAICFJJoAAAAGRBMAAMCAaAIAABgQTQAAAAOiCQAAYGBmtxwH\nAI4tO3bt2fB97t+9c8P3CRx7nGkCAAAYEE0AAAADogkAAGBANAEAAAyIJgAAgAHRBAAAMCCaAAAA\nBkQTAADAgGgCAAAYEE0AAAADogkAAGBANAEAAAyIJgAAgAHRBAAAMCCaAAAABkQTAADAgGgCAAAY\nEE0AAAADogkAAGBANAEAAAyIJgAAgAHRBAAAMCCaAAAABkQTAADAgGgCAAAY2DbvCQAAHK0du/bM\nZb/7d++cy36B+XCmCQAAYEA0AQAADIgmAACAAdEEAAAwIJoAAAAGRBMAAMCAaAIAABgQTQAAAAOi\nCQAAYGDbvCcAALDZ7Ni1Z8P3uX/3zg3fJ7DEmSYAAIAB0QQAADAgmgAAAAZEEwAAwIBoAgAAGBBN\nAAAAA6IJAABgQDQBAAAMiCYAAIAB0QQAADAgmgAAAAZEEwAAwIBoAgAAGNg27wkAAHBkO3btmct+\n9+/eOZf9wiJxpgkAAGBANAEAAAyIJgAAgAHfaQIA4LB8lwqcaQIAABgSTQAAAAOiCQAAYEA0AQAA\nDIgmAACAAdEEAAAw4JbjAAAsnHnc6txtzjkcZ5oAAAAGNk00VdW5VXVnVe2rql3zng8AAHBs2BSX\n51XVcUleneTbkhxIcmtV3djdH5rvzAAA2CrmcUngsWazXgK5Wc40nZlkX3d/pLv/Jsl1Sc6b85wA\nAIBjwKY405Tk5CQfW7Z8IMnTl69QVZckuWRa/FRV3TnY3mOSPLDKfa9m3SOtc1KST6xyf5vFWv4O\nN9P+12O7R7uNtb5utes7hg9vnsfxVjyG1/pan8UPn8/i9d+Gz+KN57N4/bezsMdxvXLhjuO/v6q1\nunvh/yS5IMkvLVv+niT/5WFs78r1XPdI6yTZO++/wxn8m6z673Az7X89tnu021jr61a7vmN4tv/e\ni7bveR7Da32tz+LF+PdexP37LD7s81vuGF6vf+9F2/d6bddxvDh/NsvleQeSnLps+ZQkdz+M7b1p\nndddy/a2inm/51ntfz22e7TbWOvrVru+Y/jw5vm+t+IxvNbX+ix++Ob9nrficeyzeOP5LF7/7TiO\n11lNxbfQqmpbkg8nOTvJnyW5Ncl3dfftc53YKlXV3u4+Y97zgKPlGGYrcByz2TmG2Qo263G8Kb7T\n1N0PVtUPJrkpyXFJrt4swTS5ct4TgIfJMcxW4Dhms3MMsxVsyuN4U5xpAgAAmJfN8p0mAACAuRBN\nAAAAA6IJAABgQDRtsKp6dFVdW1X/vaq+e97zgaNRVV9VVVdV1Q3zngscrao6f/osfmNVnTPv+cBa\nVdWTquoXq+qGqnrJvOcDR2v67+Pbqurb5z2XwxFN66Cqrq6qe6vqgw8ZP7eq7qyqfVW1axr+jiQ3\ndPf3J3nehk8WDmMtx3F3f6S7L57PTOHw1ngc//b0Wfy9Sb5zDtOFL7LGY/iO7v6BJC9Isulu4czW\ntcb/Nk6Sn0hy/cbOcm1E0/q4Jsm5yweq6rgkr07ynCSnJ3lhVZ2epV/M+7Fptc9u4BzhSK7J6o9j\nWFTXZO3H8U9Nz8MiuCZrOIar6nlJ3p7kLRs7TRi6Jqs8jqvqWUk+lOTjGz3JtRBN66C735rkvocM\nn5lk3/R/5P8myXVJzktyIEvhlPj7Z4Gs8TiGhbSW47iWvDLJ/+zud2/0XGEla/0s7u4bu/ufJnHJ\nPwtjjcfxtyY5K8l3Jfn+qlrI/z7eFL/cdpM6Of//jFKyFEtPT3JFkv9aVTuTvGkeE4M1WPE4rqoT\nk1ye5KlV9bLu/tm5zA5W53Cfxz+U5FlJHlNVX9PdvziPycEqHO6z+BlZuuz/kUnePId5wVqseBx3\n9w8mSVV9b5JPdPfn5jC3IxJNs1MrjHV3/1WSF2/0ZOAoHe44/vMkP7DRk4GjdLjj+Ios/Y8sWHSH\nO4ZvSXLLxk4FjtqKx/HnH3Rfs3FTWbuFPP21RRxIcuqy5VOS3D2nucDRchyzFTiO2ewcw2wFm/o4\nFk2zc2uS06rqiVX1iCQXJrlxznOCtXIcsxU4jtnsHMNsBZv6OBZN66Cqfj3JO5J8bVUdqKqLu/vB\nJD+Y5KYkdyS5vrtvn+c8YcRxzFbgOGazcwyzFWzF47i6+8hrAQAAHKOcaQIAABgQTQAAAAOiCQAA\nYEA0AQAADIgmAACAAdEEAAAwIJoAWEhV9dmqem9VfbCq3lRVxx/ldr6yqm5Y7/kBcOzwe5oAWEhV\n9anu/vLp8bVJPtzdl895WgAcg5xpAmAzeEeSkw8tVNW/rapbq+r9VfXT09grq+pfL1vn31fVj1bV\njqr64DR2XFX93LLX/qtp/DVV9bzp8Ruq6urp8cVV9YqqenRV7amq901nvr5zA987AHMmmgBYaFV1\nXJKzk9w4LZ+T5LQkZyZ5SpKvr6p/luS6JMtj5gVJXv+QzV2c5IHu/oYk35Dk+6vqiUnemuRbpnVO\nTnL69Pibk7wtyblJ7u7uJ3f3P0ryu+v6JgFYaKIJgEX1qKp6b5I/T/LYJDdP4+dMf96T5N1J/mGS\n07r7PUkeN32H6clJ7u/ujz5km+ckedG03XclOTFLAfa2JN9SVacn+VCSj1fVE5J8Y5I/TPKBJM+a\nzmZ9S3c/MLu3DcCi2TbvCQDAYfzf7n5KVT0mye8kuTTJFUkqyc92939b4TU3JHl+kr+XpTNPD1VJ\nfqi7b/qiJ6pOyNIZpbdmKdJekORT3f3JJJ+sqq9P8twkP1tVv9fdL3/Y7xCATcGZJgAW2nRW54eT\n/FhVfWmSm5J8X1UduknEyVX1uGn165JcmKVwWumOeTclecm0nVTVP6iqR0/PvSPJj2Qpmt6W5Mem\nn6mqr0zy6e7+1ST/McnT1v2NArCwnGkCYOF193uq6n1JLuzu11XVk5K8o6qS5FNJ/mWSe7v79qr6\niiR/1t33rLCpX0qyI8m7a+nFB5OcPz33tiTndPe+qvrTLJ1tetv03D9O8nNV9bkkf5vkJTN5owAs\nJLccBwAAGHB5HgAAwIBoAgAAGBBNAAAAA6IJAABgQDQBAAAMiCYAAIAB0QQAADAgmgAAAAb+H9T6\n/UhglYXQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fddc9558320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(14, 8)\n",
    "plt.hist(reviews, bins=np.logspace(0, 4, 25))\n",
    "plt.gca().set_xscale(\"log\")\n",
    "#plt.gca().set_yscale(\"log\")\n",
    "plt.xlabel('Reviews')\n",
    "plt.ylabel('#Recipes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_endpoint = \"http://allrecipes.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseRecipe(recipe):\n",
    "    articlesList = []\n",
    "    articles = page.find_all(\"article\")\n",
    "    for article in articles:\n",
    "        if article.h3 is not None:\n",
    "            name = ''.join(article.h3.text.split(\"\\n\"))\n",
    "            split = name.split(\" \")\n",
    "            name = \"\"\n",
    "            for part in split:\n",
    "                if part != \"\":\n",
    "                    name += part + \" \"\n",
    "            name = name[:-1]\n",
    "            if name != \"\":\n",
    "                print(name)\n",
    "                try:\n",
    "                    ratingStars = article.find(\"div\",{\"class\" : \"rating-stars\"})[\"data-ratingstars\"]\n",
    "                    nbReviews = article.find(\"format-large-number\")[\"number\"]\n",
    "                except:\n",
    "                    ratingStars = None\n",
    "                    nbReviews = None\n",
    "                link = article.find('a', href=re.compile('^/recipe/'))\n",
    "                if link is not None:\n",
    "                    print(link['href'])\n",
    "                    articlesList.append([name, ratingStars, nbReviews, link['href']])\n",
    "                else:\n",
    "                    link = article.find('a', href=re.compile('^/cook/'))\n",
    "                    if link is not None:\n",
    "                        print(link['href'])\n",
    "                        articlesList.append([name, ratingStars, nbReviews, link['href']])\n",
    "                    else:\n",
    "                        print(\"REALLY NO LINKS\")\n",
    "    return articlesList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all reviews of a recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_endpoint = \"http://allrecipes.com/recipe/getreviews/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllShortReviews(recipe_id, nbReviews):\n",
    "    params = {\n",
    "        \"recipeid\": recipe_id,\n",
    "        \"pagenumber\": 1,\n",
    "        \"pagesize\": nbReviews,\n",
    "        \"recipeType\": \"Recipe\",\n",
    "        \"sortBy\": \"MostHelpful\"\n",
    "    }\n",
    "    r = requests.get(reviews_endpoint, params=params)\n",
    "    response = BeautifulSoup(r.text, 'html.parser')\n",
    "    html_reviews = response.find_all(\"div\", {\"class\": \"review-container clearfix\"})\n",
    "    reviews = []\n",
    "    for rev in html_reviews:\n",
    "        author_ref = rev.find(\"a\")[\"href\"]\n",
    "        author_name = removeSpace(rev.find(\"h4\",{\"itemprop\":\"author\"}).text)\n",
    "        author_infos = rev.find_all(\"format-large-number\")\n",
    "        author_followers = author_infos[0][\"number\"]\n",
    "        author_favorites = author_infos[1][\"number\"]\n",
    "        author_recipes = author_infos[2][\"number\"]\n",
    "        print(rev)\n",
    "        review = {\"author_ref\": author_ref, \"author_name\": author_name, \"author_followers\": author_followers, \n",
    "                        \"author_favorites\": author_favorites, \"author_recipes\": author_recipes}\n",
    "        reviews.append(review)\n",
    "        \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRecipeReviews(endpoint, page, size, token):\n",
    "    params = {\n",
    "        \"page\": page,\n",
    "        \"pagesize\": size,\n",
    "    }\n",
    "\n",
    "    session = requests.Session()\n",
    "    r = requests.get(endpoint, params=params, headers={'Authorization': 'Bearer ' + token})\n",
    "    response = json.loads(r.text)\n",
    "    if \"reviews\" in response:\n",
    "        print(\"Retrieved reviews: \" + str(len(response[\"reviews\"])))\n",
    "        return response[\"reviews\"]\n",
    "    else:\n",
    "        print(\"No reviews\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_review = 100\n",
    "\n",
    "def getAllRecipeReviews(endpoint, size, token):\n",
    "    all_reviews = []\n",
    "    nb_reviews = 0\n",
    "    page = 1\n",
    "    while (size - nb_reviews) > max_review:\n",
    "        reviews = getRecipeReviews(endpoint, page, max_review, token)\n",
    "        nb_reviews += max_review\n",
    "        page += 1\n",
    "        if reviews is not None:\n",
    "            all_reviews = all_reviews + reviews\n",
    "        \n",
    "    reviews = getRecipeReviews(endpoint, page, (size - nb_reviews), token)\n",
    "    nb_reviews += (size - nb_reviews)\n",
    "    if reviews is not None:\n",
    "        all_reviews = all_reviews + reviews\n",
    "    \n",
    "    return all_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting photos metadatas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRecipePhotos(endpoint, page, size, token):\n",
    "    params = {\n",
    "        \"page\": page,\n",
    "        \"pagesize\": size,\n",
    "    }\n",
    "\n",
    "    session = requests.Session()\n",
    "    r = requests.get(endpoint, params=params, headers={'Authorization': 'Bearer ' + token})\n",
    "    response = json.loads(r.text)\n",
    "    if \"photos\" in response:\n",
    "        print(\"Retrieved photos: \" + str(len(response[\"photos\"])))\n",
    "        return response[\"photos\"]\n",
    "    else:\n",
    "        print(\"No photos\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_photo = 100\n",
    "\n",
    "def getAllRecipePhotos(endpoint, size, token):\n",
    "    all_photos = []\n",
    "    nb_photos = 0\n",
    "    page = 1\n",
    "    while (size - nb_photos) > max_photo:\n",
    "        photos = getRecipePhotos(endpoint, page, max_photo, token)\n",
    "        nb_photos += max_photo\n",
    "        page += 1\n",
    "        if photos is not None:\n",
    "            all_photos = all_photos + photos\n",
    "        \n",
    "    photos = getRecipePhotos(endpoint, page, (size - nb_photos), token)\n",
    "    nb_photos += (size - nb_photos)\n",
    "    if photos is not None:\n",
    "        all_photos = all_photos + photos\n",
    "    \n",
    "    return all_photos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to scrape for a Recipe\n",
    "\n",
    "Interesting properties:\n",
    "* Author\n",
    "* Description\n",
    "* \\# Made it\n",
    "* \\# Reviews\n",
    "* \\# Photos\n",
    "* Photos metadatas\n",
    "* Ingredients\n",
    "* Prep time\n",
    "* Cooking time\n",
    "* Nutrition facts\n",
    "* Number of ratings\n",
    "    * Loved it\n",
    "    * Liked it\n",
    "    * OK\n",
    "    * Didn't like it\n",
    "    * Couldn't eat it\n",
    "* Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRecipeInfos(recipe_page, recipe_data):\n",
    "    infos = recipe_page.find(\"div\",{\"class\":\"total-made-it\"})\n",
    "    recipe_data[\"made_it_count\"] = infos[\"data-ng-init\"].split(\"(\")[1].split(\",\")[0]\n",
    "    recipe_data[\"reviews_count\"] = infos.find(\"span\",{\"class\":\"review-count\"}).text.split(\" \")[0]\n",
    "    recipe_data[\"photos_count\"] = infos.find(\"format-large-number\")[\"number\"]\n",
    "\n",
    "    recipe_data[\"description\"] = recipe_page.find(\"div\", {\"class\":\"submitter__description\"}).text\n",
    "    recipe_data[\"featured_in_magazine\"] = (len(recipe_page.find_all(\"span\",{\"class\":\"recipe-summary__magazine__font\"})) > 0)\n",
    "\n",
    "    html_ingredients = recipe_page.find_all(\"span\", {\"class\":\"recipe-ingred_txt added\",\"itemprop\":\"ingredients\"})\n",
    "    ingredients = []\n",
    "    for ing in html_ingredients:\n",
    "        ingredients.append({\"id\":ing[\"data-id\"],\"text\":ing.text})\n",
    "    recipe_data[\"ingredients\"] = ingredients\n",
    "    \n",
    "    try:\n",
    "        recipe_data[\"ready_time\"] = recipe_page.find(\"span\",{\"class\":\"ready-in-time\"}).text\n",
    "    except:\n",
    "        recipe_data[\"ready_time\"] = None\n",
    "    try:\n",
    "        recipe_data[\"prep_time\"] = recipe_page.find(\"time\", {\"itemprop\":\"prepTime\"}).text\n",
    "    except:\n",
    "        recipe_data[\"prep_time\"] = None\n",
    "    try:\n",
    "        recipe_data[\"cook_time\"] = recipe_page.find(\"time\", {\"itemprop\":\"cookTime\"}).text\n",
    "    except:\n",
    "        recipe_data[\"cook_time\"] = None\n",
    "        \n",
    "    try:\n",
    "        html_nutri_facts = recipe_page.find(\"section\", {\"class\":\"ng-hide recipe-nutrition panel\"})\n",
    "        nutri_facts = {}\n",
    "        nutri_facts[\"calories\"] = html_nutri_facts.find(\"li\",{\"class\":\"nutrientLine__item--amount\", \"itemprop\":\"calories\"}).text\n",
    "        nutri_facts[\"fatContent\"] = html_nutri_facts.find(\"li\",{\"class\":\"nutrientLine__item--amount\", \"itemprop\":\"fatContent\"}).text\n",
    "        nutri_facts[\"carbohydrateContent\"] = html_nutri_facts.find(\"li\",{\"class\":\"nutrientLine__item--amount\", \"itemprop\":\"carbohydrateContent\"}).text\n",
    "        nutri_facts[\"proteinContent\"] = html_nutri_facts.find(\"li\",{\"class\":\"nutrientLine__item--amount\", \"itemprop\":\"proteinContent\"}).text\n",
    "        nutri_facts[\"cholesterolContent\"] = html_nutri_facts.find(\"li\",{\"class\":\"nutrientLine__item--amount\", \"itemprop\":\"cholesterolContent\"}).text\n",
    "        nutri_facts[\"sodiumContent\"] = html_nutri_facts.find(\"li\",{\"class\":\"nutrientLine__item--amount\", \"itemprop\":\"sodiumContent\"}).text\n",
    "        recipe_data[\"nutri_facts\"] = nutri_facts\n",
    "    except:\n",
    "        recipe_data[\"nutri_facts\"] = None\n",
    "        \n",
    "    instructions = list(map(lambda x: removeSpace(x.text), recipe_page.find_all(\"li\",{\"class\":\"step\"})))\n",
    "    instructions = list(filter(lambda x: (x != \"\" and x != None), instructions))\n",
    "    recipe_data[\"instructions\"] = instructions\n",
    "                \n",
    "    stars_rate = list(map(lambda x: x[\"style\"].split(\" \")[1].split(\"%\")[0], recipe_page.find_all(\"div\",{\"class\":\"reviewsummary--percent\"})))\n",
    "    recipe_data[\"stars_rate\"] = stars_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape a recipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrapeRecipe(recipe_link):\n",
    "\n",
    "    recipe_id = recipe_link.split(\"/\")[2]\n",
    "    \n",
    "    directory = 'recipe_data/' + recipe_id + \"/\"\n",
    "    fname = directory + \"photos.json\"\n",
    "    if os.path.isfile(fname) :\n",
    "        return False\n",
    "    \n",
    "    session = requests.Session()\n",
    "    r = requests.get(\"http://allrecipes.com\" + recipe_link)\n",
    "    recipe_page = BeautifulSoup(r.text, 'html.parser')\n",
    "    token = r.cookies.get_dict()[\"ARToken\"]\n",
    "\n",
    "    print(\"Scraping Recipe with link: \" + recipe_link)\n",
    "\n",
    "    recipe_data = {}\n",
    "    getRecipeInfos(recipe_page, recipe_data)\n",
    "    \n",
    "    if os.path.exists(directory):\n",
    "        i = 3\n",
    "    else:\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    with open('recipe_data/' + recipe_id + \"/data.json\", 'w+') as outfile:\n",
    "        json.dump(recipe_data, outfile)\n",
    "    \n",
    "         \n",
    "    reviews_endpoint = \"https://apps.allrecipes.com/v1/recipes/\"+str(recipe_id) + \"/reviews/\"\n",
    "    reviews = getAllRecipeReviews(reviews_endpoint, int(recipe_data[\"reviews_count\"]), token)\n",
    "    with open('recipe_data/' + recipe_id + \"/reviews.json\", 'w+') as outfile:\n",
    "        json.dump(reviews, outfile)\n",
    "      \n",
    "    photos_endpoint = \"https://apps.allrecipes.com/v1/recipes/\" + str(recipe_id) + \"/photos\"\n",
    "    photos = getAllRecipePhotos(photos_endpoint, int(recipe_data[\"photos_count\"]), token)\n",
    "    with open('recipe_data/' + recipe_id + \"/photos.json\", 'w+') as outfile:\n",
    "        json.dump(photos, outfile)\n",
    "        \n",
    "    print(recipe_id + \" scraped\")\n",
    "    return recipe_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Nutritional Informations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNutritionalFacts(recipe_id, token):\n",
    "    directory = 'recipe_data/' + recipe_id + \"/\"\n",
    "    fname = directory + \"nutrition.json\"\n",
    "    if os.path.isfile(fname) :\n",
    "        return False\n",
    "    \n",
    "    endpoint = \"https://apps.allrecipes.com/v1/recipes/\" + recipe_id\n",
    "    params = {\n",
    "        \"fields\": \"nutrition\",\n",
    "        \"isMetric\": \"false\",\n",
    "    }\n",
    "    r = requests.get(endpoint, params=params, headers={'Authorization': 'Bearer ' + token})\n",
    "\n",
    "    \n",
    "    try:\n",
    "        nutrition = json.loads(r.text)\n",
    "        with open('recipe_data/' + recipe_id + \"/nutrition.json\", 'w+') as outfile:\n",
    "            json.dump(nutrition, outfile)\n",
    "    except:\n",
    "        print(\"error\")\n",
    "        print(r.text)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping all recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59800"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_articles = cleaned_articles\n",
    "scraped_articles = scraped_articles.reset_index()\n",
    "del scraped_articles[\"index\"]\n",
    "len(scraped_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "for recipe in scraped_articles[\"Recipe link\"]:\n",
    "    scrapeRecipe(recipe)\n",
    "    index += 1\n",
    "    #print(\"got \" + str(index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nb_threads = 1\\nindex = 0\\n\\nnb_made = 49\\n\\nwhile index < len(scraped_articles):\\n    if (len(scraped_articles) - index) < nb_threads:\\n        nb_threads = (len(scraped_articles) - index)\\n\\n    threads = []\\n    \\n    for i in range(nb_threads):\\n        print(\"index: \" + str(i+index))\\n        t = threading.Thread(target=scrape,args=(scraped_articles[\"Recipe link\"][i+index],))\\n        t.daemon = True\\n        t.start()\\n        threads.append(t)\\n\\n    for t in threads:\\n        t.join()\\n    \\n    nb_made += 1\\n    index += nb_threads\\n    \\n    if nb_made % 50 == 0:\\n        time.sleep(5)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Scraping all recipes informations with their comments and pictures infos\n",
    "'''nb_threads = 1\n",
    "index = 0\n",
    "\n",
    "nb_made = 49\n",
    "\n",
    "while index < len(scraped_articles):\n",
    "    if (len(scraped_articles) - index) < nb_threads:\n",
    "        nb_threads = (len(scraped_articles) - index)\n",
    "\n",
    "    threads = []\n",
    "    \n",
    "    for i in range(nb_threads):\n",
    "        print(\"index: \" + str(i+index))\n",
    "        t = threading.Thread(target=scrape,args=(scraped_articles[\"Recipe link\"][i+index],))\n",
    "        t.daemon = True\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "\n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    \n",
    "    nb_made += 1\n",
    "    index += nb_threads\n",
    "    \n",
    "    if nb_made % 50 == 0:\n",
    "        time.sleep(5)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#Scraping all nutritional informations\\n\\nrecipe_ids = [f.path.split(\"/\")[1] for f in os.scandir(\"recipe_data/\") if f.is_dir() ]   \\n\\nsession = requests.Session()\\nr = requests.get(\"http://allrecipes.com\" + \"/recipe/12342/oatmeal-pie-ii/\")\\nrecipe_page = BeautifulSoup(r.text, \\'html.parser\\')\\ntoken = r.cookies.get_dict()[\"ARToken\"]\\n\\ni = 0\\nfor recipe_id in recipe_ids:\\n    getNutritionalFacts(recipe_id, token)\\n    i+=1\\n    if(i % 10000) == 0:\\n        print(\"Got \" + str(i) + \"/\" + str(len(recipe_ids)))\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#Scraping all nutritional informations\n",
    "\n",
    "recipe_ids = [f.path.split(\"/\")[1] for f in os.scandir(\"recipe_data/\") if f.is_dir() ]   \n",
    "\n",
    "session = requests.Session()\n",
    "r = requests.get(\"http://allrecipes.com\" + \"/recipe/12342/oatmeal-pie-ii/\")\n",
    "recipe_page = BeautifulSoup(r.text, 'html.parser')\n",
    "token = r.cookies.get_dict()[\"ARToken\"]\n",
    "\n",
    "i = 0\n",
    "for recipe_id in recipe_ids:\n",
    "    getNutritionalFacts(recipe_id, token)\n",
    "    i+=1\n",
    "    if(i % 10000) == 0:\n",
    "        print(\"Got \" + str(i) + \"/\" + str(len(recipe_ids)))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url_start = \"https://apps.allrecipes.com/v1/users/\"\n",
    "url_end = \"/made\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserMadeIt(page, size, user_id, token):\n",
    "    params = {\n",
    "        \"page\": 1,\n",
    "        \"pagesize\": 1000\n",
    "    }\n",
    "    r = requests.get(url_start + user_id + url_end, params=params, headers={'Authorization': 'Bearer ' + token})\n",
    "    response = json.loads(r.text)\n",
    "    \n",
    "    if \"recipes\" in response:\n",
    "        #print(\"Retrieved user made recipes: \" + str(len(response[\"recipes\"])))\n",
    "        return response[\"recipes\"]\n",
    "    else:\n",
    "        print(\"No recipes\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_made_it = 100\n",
    "\n",
    "def getAllUserMadeIt(user_id, size, token):\n",
    "    all_made_it = []\n",
    "    nb_made_it = 0\n",
    "    page = 1\n",
    "    while (size - nb_made_it) > max_made_it:\n",
    "        made_it = getUserMadeIt(page, max_made_it, user_id, token)\n",
    "        nb_made_it += max_made_it\n",
    "        page += 1\n",
    "        if made_it is not None:\n",
    "            all_made_it = all_made_it + made_it\n",
    "        \n",
    "    made_it = getUserMadeIt(page, (size - nb_made_it), user_id, token)\n",
    "    nb_made_it += (size - nb_made_it)\n",
    "    if made_it is not None:\n",
    "        all_made_it = all_made_it + made_it\n",
    "    \n",
    "    return all_made_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUserInfos(user_page, user_data):\n",
    "    infos = [\"FollowersCount\",\"FollowingCount\",\"FavoriteCount\",\"MadeRecipesCount\",\"RatingCount\",\"PersonalRecipeSharedCount\",\"SubmittedRecipesCount\"]\n",
    "    \n",
    "    for info in infos:\n",
    "        parts = str(user_page).split(info + \"\\\":\")\n",
    "        count = 0\n",
    "        for s in parts[1:]:\n",
    "            count = max(int((s.split(\",\")[0])), count)\n",
    "        user_data[info] = count    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scrapeUser(user_id):\n",
    "    \n",
    "    directory = 'user_data/' + user_id + \"/\"\n",
    "    fname = directory + \"recipes.json\"\n",
    "    if os.path.isfile(fname) :\n",
    "        return False\n",
    "    \n",
    "    session = requests.Session()\n",
    "    r = requests.get(\"http://allrecipes.com/cook/\" + user_id)\n",
    "    user_page = BeautifulSoup(r.text, 'html.parser')\n",
    "    token = r.cookies.get_dict()[\"ARToken\"]\n",
    "\n",
    "    #print(\"Scraping User : \" + user_id)\n",
    "\n",
    "    user_data = {}\n",
    "    \n",
    "    getUserInfos(user_page, user_data)\n",
    "    \n",
    "    \n",
    "    if os.path.exists(directory):\n",
    "        i = 3\n",
    "    else:\n",
    "        os.makedirs(directory)\n",
    "    \n",
    "    with open('user_data/' + user_id + \"/data.json\", 'w+') as outfile:\n",
    "        json.dump(user_data, outfile)\n",
    "         \n",
    "    made_it = getAllUserMadeIt(user_id, int(user_data[\"MadeRecipesCount\"]), token)\n",
    "    with open('user_data/' + user_id + \"/recipes.json\", 'w+') as outfile:\n",
    "        json.dump(made_it, outfile)\n",
    "        \n",
    "    #print(user_id + \" scraped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Getting list of users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['229804', '245348', '25093', '14930', '257743', '255991', '236394', '13148', '18454', '215435']\n",
      "Nb of recipes: 44069\n"
     ]
    }
   ],
   "source": [
    "recipes = [f.path.split(\"/\")[1] for f in os.scandir(\"recipe_data/\") if f.is_dir() ]    \n",
    "print(recipes[:10])\n",
    "print(\"Nb of recipes: \" + str(len(recipes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAllUsersWithReviews(start_path, files):\n",
    "    users = set()\n",
    "    for file in files:\n",
    "        try:\n",
    "            with open(start_path + file + \"/reviews.json\") as data_file:    \n",
    "                reviews = json.load(data_file)\n",
    "                for review in reviews:\n",
    "                    users.add(review[\"submitter\"][\"userID\"])\n",
    "        except: \n",
    "            print(\"Problem with file: \" + file)\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nusers = getAllUsersWithReviews(\"recipe_data/\", recipes)\\ndfUsers = pd.DataFrame(list(users))\\ndfUsers.to_csv(\"users.csv\")\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "users = getAllUsersWithReviews(\"recipe_data/\", recipes)\n",
    "dfUsers = pd.DataFrame(list(users))\n",
    "dfUsers.to_csv(\"users.csv\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = pd.read_csv(\"users.csv\")[\"user_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ni = 0\\nfor user in users:\\n    i += 1\\n    try:\\n        scrapeUser(str(user))\\n    except:\\n        print(\"error\")\\n    if i % 2 == 0:\\n        print(\"Scraped \" + str(i) + \" recipes\")\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "i = 0\n",
    "for user in users:\n",
    "    i += 1\n",
    "    try:\n",
    "        scrapeUser(str(user))\n",
    "    except:\n",
    "        print(\"error\")\n",
    "    if i % 2 == 0:\n",
    "        print(\"Scraped \" + str(i) + \" recipes\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
